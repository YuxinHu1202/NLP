(text2) C:\Users\Lenovo\Desktop\大三上\自然语言处理\跑模型\AGN-main>python main.py sst2.json
2024-12-12 09:35:19.346851: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_100.dll'; dlerror: cudart64_100.dll not found
2024-12-12 09:35:19.347758: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
Using TensorFlow backend.
config:
{'ae_epochs': 100,
 'batch_size': 32,
 'dev_path': 'SST2/test.jsonl',
 'dropout': 0.5,
 'epochs': 30,
 'epsilon': 0.05,
 'fgm_epsilon': 0.3,
 'iterations': 1,
 'learning_rate': 3e-05,
 'max_len': 40,
 'pretrained_model_dir': 'uncased_L-12_H-768_A-12',
 'save_dir': 'save3',
 'train_path': 'SST2/train.jsonl',
 'verbose': 1}
successful!
load data...
batch alignment...
previous data size: 4356
alignment data size: 4352
set tcol....
token size: 9622
done to set tcol...
train vae...
WARN:tensorflow:From D:\anaconda3\envs\text2\lib\site-packages\tensorflow_core\python\ops\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
WARN:tensorflow:From D:\anaconda3\envs\text2\lib\site-packages\tensorflow_core\python\ops\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
train size: 3072
dev size: 1280
2024-12-12 09:35:23.187725: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
2024-12-12 09:35:24.125199: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: NVIDIA GeForce MX450 major: 7 minor: 5 memoryClockRate(GHz): 1.575
pciBusID: 0000:01:00.0
2024-12-12 09:35:24.129900: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_100.dll'; dlerror: cudart64_100.dll not found
2024-12-12 09:35:24.133712: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cublas64_100.dll'; dlerror: cublas64_100.dll not found
2024-12-12 09:35:24.139117: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cufft64_100.dll'; dlerror: cufft64_100.dll not found
2024-12-12 09:35:24.145622: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'curand64_100.dll'; dlerror: curand64_100.dll not found
2024-12-12 09:35:24.151398: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cusolver64_100.dll'; dlerror: cusolver64_100.dll not found
2024-12-12 09:35:24.156276: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cusparse64_100.dll'; dlerror: cusparse64_100.dll not found
2024-12-12 09:35:24.162578: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudnn64_7.dll'; dlerror: cudnn64_7.dll not found
2024-12-12 09:35:24.162872: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1641] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2024-12-12 09:35:24.163779: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2024-12-12 09:35:24.169259: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2024-12-12 09:35:24.169663: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]
WARN:tensorflow:From D:\anaconda3\envs\text2\lib\site-packages\keras\backend\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

Train on 3072 samples, validate on 1280 samples
Epoch 1/100
 - 0s - loss: 0.6904 - val_loss: 0.6030
Epoch 2/100
 - 0s - loss: 0.5583 - val_loss: 0.5312
Epoch 3/100
 - 0s - loss: 0.5188 - val_loss: 0.5094
Epoch 4/100
 - 0s - loss: 0.5010 - val_loss: 0.4998
Epoch 5/100
 - 0s - loss: 0.4939 - val_loss: 0.4936
Epoch 6/100
 - 0s - loss: 0.4879 - val_loss: 0.4855
Epoch 7/100
 - 0s - loss: 0.4842 - val_loss: 0.4866
Epoch 8/100
 - 0s - loss: 0.4825 - val_loss: 0.4769
Epoch 9/100
 - 0s - loss: 0.4791 - val_loss: 0.4793
Epoch 10/100
 - 0s - loss: 0.4740 - val_loss: 0.4763
Epoch 11/100
 - 0s - loss: 0.4740 - val_loss: 0.4749
Epoch 12/100
 - 0s - loss: 0.4713 - val_loss: 0.4731
Epoch 13/100
 - 0s - loss: 0.4701 - val_loss: 0.4670
Epoch 14/100
 - 0s - loss: 0.4695 - val_loss: 0.4656
Epoch 15/100
 - 0s - loss: 0.4674 - val_loss: 0.4647
Epoch 16/100
 - 0s - loss: 0.4631 - val_loss: 0.4622
Epoch 17/100
 - 0s - loss: 0.4631 - val_loss: 0.4612
Epoch 18/100
 - 0s - loss: 0.4601 - val_loss: 0.4600
Epoch 19/100
 - 0s - loss: 0.4587 - val_loss: 0.4604
Epoch 20/100
 - 0s - loss: 0.4553 - val_loss: 0.4576
Epoch 21/100
 - 0s - loss: 0.4558 - val_loss: 0.4557
Epoch 22/100
 - 0s - loss: 0.4526 - val_loss: 0.4511
Epoch 23/100
 - 0s - loss: 0.4525 - val_loss: 0.4509
Epoch 24/100
 - 0s - loss: 0.4497 - val_loss: 0.4479
Epoch 25/100
 - 0s - loss: 0.4471 - val_loss: 0.4473
Epoch 26/100
 - 0s - loss: 0.4447 - val_loss: 0.4451
Epoch 27/100
 - 0s - loss: 0.4429 - val_loss: 0.4419
Epoch 28/100
 - 0s - loss: 0.4414 - val_loss: 0.4407
Epoch 29/100
 - 0s - loss: 0.4395 - val_loss: 0.4409
Epoch 30/100
 - 0s - loss: 0.4373 - val_loss: 0.4389
Epoch 31/100
 - 0s - loss: 0.4369 - val_loss: 0.4362
Epoch 32/100
 - 0s - loss: 0.4348 - val_loss: 0.4346
Epoch 33/100
 - 0s - loss: 0.4338 - val_loss: 0.4341
Epoch 34/100
 - 0s - loss: 0.4322 - val_loss: 0.4328
Epoch 35/100
 - 0s - loss: 0.4297 - val_loss: 0.4312
Epoch 36/100
 - 0s - loss: 0.4276 - val_loss: 0.4292
Epoch 37/100
 - 0s - loss: 0.4256 - val_loss: 0.4276
Epoch 38/100
 - 0s - loss: 0.4265 - val_loss: 0.4263
Epoch 39/100
 - 0s - loss: 0.4248 - val_loss: 0.4255
Epoch 40/100
 - 0s - loss: 0.4239 - val_loss: 0.4234
Epoch 41/100
 - 0s - loss: 0.4228 - val_loss: 0.4248
Epoch 42/100
 - 0s - loss: 0.4221 - val_loss: 0.4243
Epoch 43/100
 - 0s - loss: 0.4214 - val_loss: 0.4220
Epoch 44/100
 - 0s - loss: 0.4211 - val_loss: 0.4209
Epoch 45/100
 - 0s - loss: 0.4201 - val_loss: 0.4230
Epoch 46/100
 - 0s - loss: 0.4193 - val_loss: 0.4220
Epoch 47/100
 - 0s - loss: 0.4186 - val_loss: 0.4221
Epoch 48/100
 - 0s - loss: 0.4191 - val_loss: 0.4198
Epoch 49/100
 - 0s - loss: 0.4182 - val_loss: 0.4211
Epoch 50/100
 - 0s - loss: 0.4181 - val_loss: 0.4215
Epoch 51/100
 - 0s - loss: 0.4181 - val_loss: 0.4210
Epoch 52/100
 - 0s - loss: 0.4180 - val_loss: 0.4194
Epoch 53/100
 - 0s - loss: 0.4176 - val_loss: 0.4194
Epoch 54/100
 - 0s - loss: 0.4171 - val_loss: 0.4179
Epoch 55/100
 - 0s - loss: 0.4157 - val_loss: 0.4174
Epoch 56/100
 - 0s - loss: 0.4159 - val_loss: 0.4172
Epoch 57/100
 - 0s - loss: 0.4150 - val_loss: 0.4163
Epoch 58/100
 - 0s - loss: 0.4147 - val_loss: 0.4183
Epoch 59/100
 - 0s - loss: 0.4139 - val_loss: 0.4158
Epoch 60/100
 - 0s - loss: 0.4146 - val_loss: 0.4146
Epoch 61/100
 - 0s - loss: 0.4130 - val_loss: 0.4136
Epoch 62/100
 - 0s - loss: 0.4124 - val_loss: 0.4136
Epoch 63/100
 - 0s - loss: 0.4124 - val_loss: 0.4137
Epoch 64/100
 - 0s - loss: 0.4122 - val_loss: 0.4126
Epoch 65/100
 - 0s - loss: 0.4115 - val_loss: 0.4138
Epoch 66/100
 - 0s - loss: 0.4112 - val_loss: 0.4126
Epoch 67/100
 - 0s - loss: 0.4110 - val_loss: 0.4114
Epoch 68/100
 - 0s - loss: 0.4100 - val_loss: 0.4119
Epoch 69/100
 - 0s - loss: 0.4103 - val_loss: 0.4114
Epoch 70/100
 - 0s - loss: 0.4093 - val_loss: 0.4114
Epoch 71/100
 - 0s - loss: 0.4085 - val_loss: 0.4115
Epoch 72/100
 - 0s - loss: 0.4094 - val_loss: 0.4110
Epoch 73/100
 - 0s - loss: 0.4093 - val_loss: 0.4111
Epoch 74/100
 - 0s - loss: 0.4089 - val_loss: 0.4100
Epoch 75/100
 - 0s - loss: 0.4088 - val_loss: 0.4099
Epoch 76/100
 - 0s - loss: 0.4092 - val_loss: 0.4105
Epoch 77/100
 - 0s - loss: 0.4084 - val_loss: 0.4102
Epoch 78/100
 - 0s - loss: 0.4085 - val_loss: 0.4086
Epoch 79/100
 - 0s - loss: 0.4082 - val_loss: 0.4092
Epoch 80/100
 - 0s - loss: 0.4080 - val_loss: 0.4094
Epoch 81/100
 - 0s - loss: 0.4084 - val_loss: 0.4095
Epoch 82/100
 - 0s - loss: 0.4082 - val_loss: 0.4097
Epoch 83/100
 - 0s - loss: 0.4078 - val_loss: 0.4096
batch alignment...
previous data size: 991
alignment data size: 960
train vae...
build generator
Skip Embedding-Mapping
Model: "model_4"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
Input-Token (InputLayer)        (None, None)         0
__________________________________________________________________________________________________
Input-Segment (InputLayer)      (None, None)         0
__________________________________________________________________________________________________
Embedding-Token (TokenEmbedding [(None, None, 768),  23440896    Input-Token[0][0]
__________________________________________________________________________________________________
Embedding-Segment (Embedding)   (None, None, 768)    1536        Input-Segment[0][0]
__________________________________________________________________________________________________
Embedding-Token-Segment (Add)   (None, None, 768)    0           Embedding-Token[0][0]
                                                                 Embedding-Segment[0][0]
__________________________________________________________________________________________________
Embedding-Position (AbsolutePos (None, None, 768)    393216      Embedding-Token-Segment[0][0]
__________________________________________________________________________________________________
Embedding-Norm (LayerNorm)      (None, None, 768)    1536        Embedding-Position[0][0]
__________________________________________________________________________________________________
Embedding-Dropout (Dropout)     (None, None, 768)    0           Embedding-Norm[0][0]
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    2362368     Embedding-Dropout[0][0]
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    0           Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    0           Embedding-Dropout[0][0]
                                                                 Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-FeedForward (Feed (None, None, 768)    4722432     Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-FeedForward-Dropo (None, None, 768)    0           Transformer-1-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-1-FeedForward-Add ( (None, None, 768)    0           Transformer-1-MultiHeadSelfAttent
                                                                 Transformer-1-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-1-FeedForward-Norm  (None, None, 768)    1536        Transformer-1-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-1-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    0           Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    0           Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-FeedForward (Feed (None, None, 768)    4722432     Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-FeedForward-Dropo (None, None, 768)    0           Transformer-2-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-2-FeedForward-Add ( (None, None, 768)    0           Transformer-2-MultiHeadSelfAttent
                                                                 Transformer-2-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-2-FeedForward-Norm  (None, None, 768)    1536        Transformer-2-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-2-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    0           Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    0           Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-FeedForward (Feed (None, None, 768)    4722432     Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-FeedForward-Dropo (None, None, 768)    0           Transformer-3-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-3-FeedForward-Add ( (None, None, 768)    0           Transformer-3-MultiHeadSelfAttent
                                                                 Transformer-3-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-3-FeedForward-Norm  (None, None, 768)    1536        Transformer-3-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-3-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    0           Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    0           Transformer-3-FeedForward-Norm[0]
                                                                 Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-FeedForward (Feed (None, None, 768)    4722432     Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-FeedForward-Dropo (None, None, 768)    0           Transformer-4-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-4-FeedForward-Add ( (None, None, 768)    0           Transformer-4-MultiHeadSelfAttent
                                                                 Transformer-4-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-4-FeedForward-Norm  (None, None, 768)    1536        Transformer-4-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-4-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    0           Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    0           Transformer-4-FeedForward-Norm[0]
                                                                 Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-FeedForward (Feed (None, None, 768)    4722432     Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-FeedForward-Dropo (None, None, 768)    0           Transformer-5-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-5-FeedForward-Add ( (None, None, 768)    0           Transformer-5-MultiHeadSelfAttent
                                                                 Transformer-5-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-5-FeedForward-Norm  (None, None, 768)    1536        Transformer-5-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-5-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 768)    0           Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 768)    0           Transformer-5-FeedForward-Norm[0]
                                                                 Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-FeedForward (Feed (None, None, 768)    4722432     Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-FeedForward-Dropo (None, None, 768)    0           Transformer-6-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-6-FeedForward-Add ( (None, None, 768)    0           Transformer-6-MultiHeadSelfAttent
                                                                 Transformer-6-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-6-FeedForward-Norm  (None, None, 768)    1536        Transformer-6-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-6-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 768)    0           Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 768)    0           Transformer-6-FeedForward-Norm[0]
                                                                 Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-FeedForward (Feed (None, None, 768)    4722432     Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-FeedForward-Dropo (None, None, 768)    0           Transformer-7-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-7-FeedForward-Add ( (None, None, 768)    0           Transformer-7-MultiHeadSelfAttent
                                                                 Transformer-7-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-7-FeedForward-Norm  (None, None, 768)    1536        Transformer-7-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-7-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 768)    0           Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 768)    0           Transformer-7-FeedForward-Norm[0]
                                                                 Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-FeedForward (Feed (None, None, 768)    4722432     Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-FeedForward-Dropo (None, None, 768)    0           Transformer-8-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-8-FeedForward-Add ( (None, None, 768)    0           Transformer-8-MultiHeadSelfAttent
                                                                 Transformer-8-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-8-FeedForward-Norm  (None, None, 768)    1536        Transformer-8-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-8-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 768)    0           Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 768)    0           Transformer-8-FeedForward-Norm[0]
                                                                 Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-FeedForward (Feed (None, None, 768)    4722432     Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-FeedForward-Dropo (None, None, 768)    0           Transformer-9-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-9-FeedForward-Add ( (None, None, 768)    0           Transformer-9-MultiHeadSelfAttent
                                                                 Transformer-9-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-9-FeedForward-Norm  (None, None, 768)    1536        Transformer-9-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 768)    2362368     Transformer-9-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 768)    0           Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 768)    0           Transformer-9-FeedForward-Norm[0]
                                                                 Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 768)    1536        Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-FeedForward (Fee (None, None, 768)    4722432     Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-FeedForward-Drop (None, None, 768)    0           Transformer-10-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-10-FeedForward-Add  (None, None, 768)    0           Transformer-10-MultiHeadSelfAtten
                                                                 Transformer-10-FeedForward-Dropou
__________________________________________________________________________________________________
Transformer-10-FeedForward-Norm (None, None, 768)    1536        Transformer-10-FeedForward-Add[0]
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 768)    2362368     Transformer-10-FeedForward-Norm[0
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 768)    0           Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 768)    0           Transformer-10-FeedForward-Norm[0
                                                                 Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 768)    1536        Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-FeedForward (Fee (None, None, 768)    4722432     Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-FeedForward-Drop (None, None, 768)    0           Transformer-11-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-11-FeedForward-Add  (None, None, 768)    0           Transformer-11-MultiHeadSelfAtten
                                                                 Transformer-11-FeedForward-Dropou
__________________________________________________________________________________________________
Transformer-11-FeedForward-Norm (None, None, 768)    1536        Transformer-11-FeedForward-Add[0]
__________________________________________________________________________________________________
Transformer-12-MultiHeadSelfAtt (None, None, 768)    2362368     Transformer-11-FeedForward-Norm[0
__________________________________________________________________________________________________
Transformer-12-MultiHeadSelfAtt (None, None, 768)    0           Transformer-12-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-12-MultiHeadSelfAtt (None, None, 768)    0           Transformer-11-FeedForward-Norm[0
                                                                 Transformer-12-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-12-MultiHeadSelfAtt (None, None, 768)    1536        Transformer-12-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-12-FeedForward (Fee (None, None, 768)    4722432     Transformer-12-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-12-FeedForward-Drop (None, None, 768)    0           Transformer-12-FeedForward[0][0]
__________________________________________________________________________________________________
gi (InputLayer)                 (None, 40)           0
__________________________________________________________________________________________________
Transformer-12-FeedForward-Add  (None, None, 768)    0           Transformer-12-MultiHeadSelfAtten
                                                                 Transformer-12-FeedForward-Dropou
__________________________________________________________________________________________________
dense_5 (Dense)                 (None, 40)           1640        gi[0][0]
__________________________________________________________________________________________________
Transformer-12-FeedForward-Norm (None, None, 768)    1536        Transformer-12-FeedForward-Add[0]
__________________________________________________________________________________________________
lambda_2 (Lambda)               (None, 40, 1)        0           dense_5[0][0]
__________________________________________________________________________________________________
agn_1 (AGN)                     [(None, None, 768),  0           Transformer-12-FeedForward-Norm[0
                                                                 lambda_2[0][0]
__________________________________________________________________________________________________
lambda_1 (Lambda)               (None, None, 1)      0           Input-Token[0][0]
__________________________________________________________________________________________________
lambda_3 (Lambda)               (None, None, 768)    0           agn_1[0][0]
                                                                 lambda_1[0][0]
__________________________________________________________________________________________________
lambda_4 (Lambda)               (None, 768)          0           lambda_3[0][0]
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 768)          0           lambda_4[0][0]
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 768)          0           lambda_4[0][0]
__________________________________________________________________________________________________
__________________________________________________________________________________________________
dense_8 (Dense)                 (None, 3)            2307        dropout_1[0][0]
==================================================================================================
Total params: 108,895,595
Trainable params: 108,895,595
Non-trainable params: 0
__________________________________________________________________________________________________
apply fgm
start to fitting...
Epoch 1/30
136/136 [==============================] - 1894s 14s/step - loss: 1.3582
- val_acc 0.4114583333333333 - val_f1 0.32316102526367424
new best model, save model to  save3\clf_model.weights...
Epoch 2/30
136/136 [==============================] - 2051s 15s/step - loss: 1.2415
- val_acc 0.42291666666666666 - val_f1 0.29170057465687704
Epoch 3/30
Non-trainable params: 0
__________________________________________________________________________________________________
apply fgm
start to fitting...
Epoch 1/30
136/136 [==============================] - 1894s 14s/step - loss: 1.3582
- val_acc 0.4114583333333333 - val_f1 0.32316102526367424
new best model, save model to  save3\clf_model.weights...
Epoch 2/30
136/136 [==============================] - 2051s 15s/step - loss: 1.2415
- val_acc 0.42291666666666666 - val_f1 0.29170057465687704
Epoch 3/30
start to fitting...
Epoch 1/30
136/136 [==============================] - 1894s 14s/step - loss: 1.3582
- val_acc 0.4114583333333333 - val_f1 0.32316102526367424
new best model, save model to  save3\clf_model.weights...
Epoch 2/30
136/136 [==============================] - 2051s 15s/step - loss: 1.2415
- val_acc 0.42291666666666666 - val_f1 0.29170057465687704
Epoch 3/30
136/136 [==============================] - 2051s 15s/step - loss: 1.2415
- val_acc 0.42291666666666666 - val_f1 0.29170057465687704
Epoch 3/30
Epoch 3/30
136/136 [==============================] - 1764s 13s/step - loss: 1.1652
- val_acc 0.41041666666666665 - val_f1 0.2971809179935799
Epoch 4/30
136/136 [==============================] - 1840s 14s/step - loss: 1.1522
- val_acc 0.38958333333333334 - val_f1 0.27996358323900195
Epoch 5/30
136/136 [==============================] - 1871s 14s/step - loss: 1.1497
- val_acc 0.39375 - val_f1 0.2836658787541713
Epoch 6/30
136/136 [==============================] - 1874s 14s/step - loss: 1.1383
- val_acc 0.390625 - val_f1 0.28310694403897213
Epoch 7/30
136/136 [==============================] - 1957s 14s/step - loss: 1.1293
136/136 [==============================] - 1764s 13s/step - loss: 1.1652
- val_acc 0.41041666666666665 - val_f1 0.2971809179935799
Epoch 4/30
136/136 [==============================] - 1840s 14s/step - loss: 1.1522
- val_acc 0.38958333333333334 - val_f1 0.27996358323900195
Epoch 5/30
136/136 [==============================] - 1871s 14s/step - loss: 1.1497
- val_acc 0.39375 - val_f1 0.2836658787541713
Epoch 6/30
136/136 [==============================] - 1874s 14s/step - loss: 1.1383
- val_acc 0.390625 - val_f1 0.28310694403897213
Epoch 7/30
136/136 [==============================] - 1957s 14s/step - loss: 1.1293
- val_acc 0.375 - val_f1 0.2624563666423703
Epoch 8/30
136/136 [==============================] - 1740s 13s/step - loss: 1.1299
- val_acc 0.38333333333333336 - val_f1 0.2643031281738448
Epoch 4/30
136/136 [==============================] - 1840s 14s/step - loss: 1.1522
- val_acc 0.38958333333333334 - val_f1 0.27996358323900195
Epoch 5/30
136/136 [==============================] - 1871s 14s/step - loss: 1.1497
- val_acc 0.39375 - val_f1 0.2836658787541713
Epoch 6/30
136/136 [==============================] - 1874s 14s/step - loss: 1.1383
- val_acc 0.390625 - val_f1 0.28310694403897213
Epoch 7/30
136/136 [==============================] - 1957s 14s/step - loss: 1.1293
- val_acc 0.375 - val_f1 0.2624563666423703
Epoch 8/30
136/136 [==============================] - 1740s 13s/step - loss: 1.1299
- val_acc 0.38333333333333336 - val_f1 0.2643031281738448
Epoch 9/30
136/136 [==============================] - 1672s 12s/step - loss: 1.1097
- val_acc 0.36770833333333336 - val_f1 0.26158146180556713
Epoch 10/30
136/136 [==============================] - 1916s 14s/step - loss: 1.1034
- val_acc 0.365625 - val_f1 0.27297429495904524
Epoch 11/30
136/136 [==============================] - 1825s 13s/step - loss: 1.1007
- val_acc 0.3541666666666667 - val_f1 0.2656182640683303
Epoch 00011: early stopping
iteration 1 accuracy: 0.42291666666666666, f1: 0.32316102526367424

Average accuracy: 0.42291666666666666
Average f1: 0.32316102526367424