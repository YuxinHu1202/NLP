(text2) C:\Users\Lenovo\Desktop\大三上\自然语言处理\跑模型\AGN-main>python main.py sst2.json
2024-12-12 17:05:05.916154: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_100.dll'; dlerror: cudart64_100.dll not found
2024-12-12 17:05:05.916333: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
Using TensorFlow backend.
config:
{'ae_epochs': 100,
 'batch_size': 32,
 'dev_path': 'SST2/test.jsonl',
 'dropout': 0.5,
 'epochs': 10,
 'epsilon': 0.05,
 'fgm_epsilon': 0.3,
 'iterations': 1,
 'learning_rate': 3e-05,
 'max_len': 80,
 'pretrained_model_dir': 'uncased_L-12_H-768_A-12',
 'save_dir': 'save',
 'train_path': 'SST2/train.jsonl',
 'verbose': 1}
successful!
load data...
batch alignment...
previous data size: 1000
alignment data size: 992
set tcol....
token size: 5148
done to set tcol...
train vae...
WARN:tensorflow:From D:\anaconda3\envs\text2\lib\site-packages\tensorflow_core\python\ops\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
WARN:tensorflow:From D:\anaconda3\envs\text2\lib\site-packages\tensorflow_core\python\ops\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
train size: 704
dev size: 288
Train on 704 samples, validate on 288 samples
Epoch 1/100
2024-12-12 17:05:09.447523: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
2024-12-12 17:05:10.385532: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: NVIDIA GeForce MX450 major: 7 minor: 5 memoryClockRate(GHz): 1.575
pciBusID: 0000:01:00.0
2024-12-12 17:05:10.389108: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_100.dll'; dlerror: cudart64_100.dll not found   
2024-12-12 17:05:10.392435: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cublas64_100.dll'; dlerror: cublas64_100.dll not found   
2024-12-12 17:05:10.396396: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cufft64_100.dll'; dlerror: cufft64_100.dll not found     
2024-12-12 17:05:10.401032: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'curand64_100.dll'; dlerror: curand64_100.dll not found   
2024-12-12 17:05:10.405522: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cusolver64_100.dll'; dlerror: cusolver64_100.dll not found
2024-12-12 17:05:10.409781: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cusparse64_100.dll'; dlerror: cusparse64_100.dll not found
2024-12-12 17:05:10.413466: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudnn64_7.dll'; dlerror: cudnn64_7.dll not found
2024-12-12 17:05:10.413603: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1641] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.     
Skipping registering GPU devices...
2024-12-12 17:05:10.414243: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2024-12-12 17:05:10.417956: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2024-12-12 17:05:10.418141: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]
704/704 - 1s - loss: 0.7588 - val_loss: 0.7135
Epoch 2/100
704/704 - 0s - loss: 0.6653 - val_loss: 0.6180
Epoch 3/100
704/704 - 0s - loss: 0.5775 - val_loss: 0.5416
Epoch 4/100
704/704 - 0s - loss: 0.5256 - val_loss: 0.5067
Epoch 5/100
704/704 - 0s - loss: 0.4903 - val_loss: 0.4786
Epoch 6/100
704/704 - 0s - loss: 0.4700 - val_loss: 0.4613
Epoch 7/100
704/704 - 0s - loss: 0.4580 - val_loss: 0.4548
Epoch 8/100
704/704 - 0s - loss: 0.4462 - val_loss: 0.4417
Epoch 9/100
704/704 - 0s - loss: 0.4340 - val_loss: 0.4367
Epoch 10/100
704/704 - 0s - loss: 0.4245 - val_loss: 0.4252
Epoch 11/100
704/704 - 0s - loss: 0.4205 - val_loss: 0.4197
Epoch 12/100
704/704 - 0s - loss: 0.4166 - val_loss: 0.4188
Epoch 13/100
704/704 - 0s - loss: 0.4102 - val_loss: 0.4160
Epoch 14/100
704/704 - 0s - loss: 0.4090 - val_loss: 0.4044
Epoch 15/100
704/704 - 0s - loss: 0.4045 - val_loss: 0.4067
Epoch 16/100
704/704 - 0s - loss: 0.4002 - val_loss: 0.3994
Epoch 17/100
704/704 - 0s - loss: 0.4014 - val_loss: 0.3962
Epoch 18/100
704/704 - 0s - loss: 0.3977 - val_loss: 0.4025
Epoch 19/100
704/704 - 0s - loss: 0.3969 - val_loss: 0.3965
Epoch 20/100
704/704 - 0s - loss: 0.3912 - val_loss: 0.3941
Epoch 21/100
704/704 - 0s - loss: 0.3922 - val_loss: 0.4017
Epoch 22/100
704/704 - 0s - loss: 0.3939 - val_loss: 0.3901
Epoch 23/100
704/704 - 0s - loss: 0.3888 - val_loss: 0.3884
Epoch 24/100
704/704 - 0s - loss: 0.3877 - val_loss: 0.3900
Epoch 25/100
704/704 - 0s - loss: 0.3839 - val_loss: 0.3909
Epoch 26/100
704/704 - 0s - loss: 0.3859 - val_loss: 0.3829
Epoch 27/100
704/704 - 0s - loss: 0.3818 - val_loss: 0.3868
Epoch 28/100
704/704 - 0s - loss: 0.3855 - val_loss: 0.3864
Epoch 29/100
704/704 - 0s - loss: 0.3819 - val_loss: 0.3870
Epoch 30/100
704/704 - 0s - loss: 0.3802 - val_loss: 0.3828
Epoch 31/100
704/704 - 0s - loss: 0.3850 - val_loss: 0.3822
Epoch 32/100
704/704 - 0s - loss: 0.3782 - val_loss: 0.3858
Epoch 33/100
704/704 - 0s - loss: 0.3781 - val_loss: 0.3878
Epoch 34/100
704/704 - 0s - loss: 0.3791 - val_loss: 0.3834
Epoch 35/100
704/704 - 0s - loss: 0.3766 - val_loss: 0.3754
Epoch 36/100
704/704 - 0s - loss: 0.3779 - val_loss: 0.3751
Epoch 37/100
704/704 - 0s - loss: 0.3751 - val_loss: 0.3820
Epoch 38/100
704/704 - 0s - loss: 0.3745 - val_loss: 0.3857
Epoch 39/100
704/704 - 0s - loss: 0.3745 - val_loss: 0.3787
Epoch 40/100
704/704 - 0s - loss: 0.3738 - val_loss: 0.3792
Epoch 41/100
704/704 - 0s - loss: 0.3733 - val_loss: 0.3750
Epoch 42/100
704/704 - 0s - loss: 0.3735 - val_loss: 0.3768
Epoch 43/100
704/704 - 0s - loss: 0.3688 - val_loss: 0.3737
Epoch 44/100
704/704 - 0s - loss: 0.3730 - val_loss: 0.3791
Epoch 45/100
704/704 - 0s - loss: 0.3715 - val_loss: 0.3833
Epoch 46/100
704/704 - 0s - loss: 0.3749 - val_loss: 0.3760
Epoch 47/100
704/704 - 0s - loss: 0.3696 - val_loss: 0.3802
Epoch 48/100
704/704 - 0s - loss: 0.3691 - val_loss: 0.3717
Epoch 49/100
704/704 - 0s - loss: 0.3672 - val_loss: 0.3777
Epoch 50/100
704/704 - 0s - loss: 0.3653 - val_loss: 0.3774
Epoch 51/100
704/704 - 0s - loss: 0.3703 - val_loss: 0.3756
Epoch 52/100
704/704 - 0s - loss: 0.3680 - val_loss: 0.3767
Epoch 53/100
704/704 - 0s - loss: 0.3697 - val_loss: 0.3800
batch alignment...
previous data size: 199
alignment data size: 192
train vae...
build generator
WARN:tensorflow:From D:\anaconda3\envs\text2\lib\site-packages\langml\plm\bert.py:64: The name tf.keras.initializers.TruncatedNormal is deprecated. Please use tf.compat.v1.keras.initializers.TruncatedNormal instead.

WARN:tensorflow:From D:\anaconda3\envs\text2\lib\site-packages\tensorflow_core\python\keras\initializers.py:94: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
Skip Embedding-Mapping
Model: "model_3"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
Input-Token (InputLayer)        [(None, None)]       0
__________________________________________________________________________________________________
Input-Segment (InputLayer)      [(None, None)]       0
__________________________________________________________________________________________________
Embedding-Token (TokenEmbedding [(None, None, 768),  23440896    Input-Token[0][0]
__________________________________________________________________________________________________
Embedding-Segment (Embedding)   (None, None, 768)    1536        Input-Segment[0][0]
__________________________________________________________________________________________________
Embedding-Token-Segment (Add)   (None, None, 768)    0           Embedding-Token[0][0]
                                                                 Embedding-Segment[0][0]
__________________________________________________________________________________________________
Embedding-Position (AbsolutePos (None, None, 768)    393216      Embedding-Token-Segment[0][0]
__________________________________________________________________________________________________
Embedding-Norm (LayerNorm)      (None, None, 768)    1536        Embedding-Position[0][0]
__________________________________________________________________________________________________
Embedding-Dropout (Dropout)     (None, None, 768)    0           Embedding-Norm[0][0]
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    2362368     Embedding-Dropout[0][0]
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    0           Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    0           Embedding-Dropout[0][0]
                                                                 Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-FeedForward (Feed (None, None, 768)    4722432     Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-FeedForward-Dropo (None, None, 768)    0           Transformer-1-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-1-FeedForward-Add ( (None, None, 768)    0           Transformer-1-MultiHeadSelfAttent
                                                                 Transformer-1-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-1-FeedForward-Norm  (None, None, 768)    1536        Transformer-1-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-1-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    0           Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    0           Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-FeedForward (Feed (None, None, 768)    4722432     Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-FeedForward-Dropo (None, None, 768)    0           Transformer-2-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-2-FeedForward-Add ( (None, None, 768)    0           Transformer-2-MultiHeadSelfAttent
                                                                 Transformer-2-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-2-FeedForward-Norm  (None, None, 768)    1536        Transformer-2-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-2-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    0           Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    0           Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-FeedForward (Feed (None, None, 768)    4722432     Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-FeedForward-Dropo (None, None, 768)    0           Transformer-3-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-3-FeedForward-Add ( (None, None, 768)    0           Transformer-3-MultiHeadSelfAttent
                                                                 Transformer-3-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-3-FeedForward-Norm  (None, None, 768)    1536        Transformer-3-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-3-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    0           Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    0           Transformer-3-FeedForward-Norm[0]
                                                                 Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-FeedForward (Feed (None, None, 768)    4722432     Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-FeedForward-Dropo (None, None, 768)    0           Transformer-4-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-4-FeedForward-Add ( (None, None, 768)    0           Transformer-4-MultiHeadSelfAttent
                                                                 Transformer-4-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-4-FeedForward-Norm  (None, None, 768)    1536        Transformer-4-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-4-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    0           Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    0           Transformer-4-FeedForward-Norm[0]
                                                                 Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-FeedForward (Feed (None, None, 768)    4722432     Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-FeedForward-Dropo (None, None, 768)    0           Transformer-5-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-5-FeedForward-Add ( (None, None, 768)    0           Transformer-5-MultiHeadSelfAttent
                                                                 Transformer-5-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-5-FeedForward-Norm  (None, None, 768)    1536        Transformer-5-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-5-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 768)    0           Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 768)    0           Transformer-5-FeedForward-Norm[0]
                                                                 Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-FeedForward (Feed (None, None, 768)    4722432     Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-FeedForward-Dropo (None, None, 768)    0           Transformer-6-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-6-FeedForward-Add ( (None, None, 768)    0           Transformer-6-MultiHeadSelfAttent
                                                                 Transformer-6-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-6-FeedForward-Norm  (None, None, 768)    1536        Transformer-6-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-6-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 768)    0           Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 768)    0           Transformer-6-FeedForward-Norm[0]
                                                                 Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-FeedForward (Feed (None, None, 768)    4722432     Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-FeedForward-Dropo (None, None, 768)    0           Transformer-7-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-7-FeedForward-Add ( (None, None, 768)    0           Transformer-7-MultiHeadSelfAttent
                                                                 Transformer-7-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-7-FeedForward-Norm  (None, None, 768)    1536        Transformer-7-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-7-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 768)    0           Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 768)    0           Transformer-7-FeedForward-Norm[0]
                                                                 Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-FeedForward (Feed (None, None, 768)    4722432     Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-FeedForward-Dropo (None, None, 768)    0           Transformer-8-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-8-FeedForward-Add ( (None, None, 768)    0           Transformer-8-MultiHeadSelfAttent
                                                                 Transformer-8-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-8-FeedForward-Norm  (None, None, 768)    1536        Transformer-8-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-8-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 768)    0           Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 768)    0           Transformer-8-FeedForward-Norm[0]
                                                                 Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-FeedForward (Feed (None, None, 768)    4722432     Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-FeedForward-Dropo (None, None, 768)    0           Transformer-9-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-9-FeedForward-Add ( (None, None, 768)    0           Transformer-9-MultiHeadSelfAttent
                                                                 Transformer-9-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-9-FeedForward-Norm  (None, None, 768)    1536        Transformer-9-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 768)    2362368     Transformer-9-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 768)    0           Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 768)    0           Transformer-9-FeedForward-Norm[0]
                                                                 Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 768)    1536        Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-FeedForward (Fee (None, None, 768)    4722432     Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-FeedForward-Drop (None, None, 768)    0           Transformer-10-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-10-FeedForward-Add  (None, None, 768)    0           Transformer-10-MultiHeadSelfAtten
                                                                 Transformer-10-FeedForward-Dropou
__________________________________________________________________________________________________
Transformer-10-FeedForward-Norm (None, None, 768)    1536        Transformer-10-FeedForward-Add[0]
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 768)    2362368     Transformer-10-FeedForward-Norm[0
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 768)    0           Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 768)    0           Transformer-10-FeedForward-Norm[0
                                                                 Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 768)    1536        Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-FeedForward (Fee (None, None, 768)    4722432     Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-FeedForward-Drop (None, None, 768)    0           Transformer-11-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-11-FeedForward-Add  (None, None, 768)    0           Transformer-11-MultiHeadSelfAtten
                                                                 Transformer-11-FeedForward-Dropou
__________________________________________________________________________________________________
Transformer-11-FeedForward-Norm (None, None, 768)    1536        Transformer-11-FeedForward-Add[0]
__________________________________________________________________________________________________
Transformer-12-MultiHeadSelfAtt (None, None, 768)    2362368     Transformer-11-FeedForward-Norm[0
__________________________________________________________________________________________________
Transformer-12-MultiHeadSelfAtt (None, None, 768)    0           Transformer-12-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-12-MultiHeadSelfAtt (None, None, 768)    0           Transformer-11-FeedForward-Norm[0
                                                                 Transformer-12-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-12-MultiHeadSelfAtt (None, None, 768)    1536        Transformer-12-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-12-FeedForward (Fee (None, None, 768)    4722432     Transformer-12-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-12-FeedForward-Drop (None, None, 768)    0           Transformer-12-FeedForward[0][0] 
__________________________________________________________________________________________________
gi (InputLayer)                 [(None, 80)]         0
__________________________________________________________________________________________________
Transformer-12-FeedForward-Add  (None, None, 768)    0           Transformer-12-MultiHeadSelfAtten
                                                                 Transformer-12-FeedForward-Dropou
__________________________________________________________________________________________________
dense_4 (Dense)                 (None, 80)           6480        gi[0][0]
__________________________________________________________________________________________________
Transformer-12-FeedForward-Norm (None, None, 768)    1536        Transformer-12-FeedForward-Add[0]
__________________________________________________________________________________________________
lambda_1 (Lambda)               (None, 80, 1)        0           dense_4[0][0]
__________________________________________________________________________________________________
agn (AGN)                       [(None, 80, 768), (N 0           Transformer-12-FeedForward-Norm[0
                                                                 lambda_1[0][0]
__________________________________________________________________________________________________
lambda (Lambda)                 (None, None, 1)      0           Input-Token[0][0]
__________________________________________________________________________________________________
lambda_2 (Lambda)               (None, 80, 768)      0           agn[0][0]
                                                                 lambda[0][0]
__________________________________________________________________________________________________
lambda_3 (Lambda)               (None, 768)          0           lambda_2[0][0]
__________________________________________________________________________________________________
dropout (Dropout)               (None, 768)          0           lambda_3[0][0]
__________________________________________________________________________________________________
dense_7 (Dense)                 (None, 2)            1538        dropout[0][0]
==================================================================================================
Total params: 108,899,666
Trainable params: 108,899,666
Non-trainable params: 0
__________________________________________________________________________________________________
apply fgm
start to fitting...
Epoch 1/10
30/31 [============================>.] - ETA: 23s - loss: 1.2200- val_acc 0.46875 - val_f1 0.4301012687696427
new best model, save model to  save\clf_model.weights...
31/31 [==============================] - 774s 25s/step - loss: 1.2126
Epoch 2/10
30/31 [============================>.] - ETA: 29s - loss: 0.9212- val_acc 0.65625 - val_f1 0.653846153846154
lambda_3 (Lambda)               (None, 768)          0           lambda_2[0][0]
__________________________________________________________________________________________________
dropout (Dropout)               (None, 768)          0           lambda_3[0][0]
__________________________________________________________________________________________________
dense_7 (Dense)                 (None, 2)            1538        dropout[0][0]
==================================================================================================
Total params: 108,899,666
Trainable params: 108,899,666
Non-trainable params: 0
__________________________________________________________________________________________________
apply fgm
start to fitting...
Epoch 1/10
30/31 [============================>.] - ETA: 23s - loss: 1.2200- val_acc 0.46875 - val_f1 0.4301012687696427
new best model, save model to  save\clf_model.weights...
31/31 [==============================] - 774s 25s/step - loss: 1.2126
Epoch 2/10
30/31 [============================>.] - ETA: 29s - loss: 0.9212- val_acc 0.65625 - val_f1 0.653846153846154
__________________________________________________________________________________________________
dropout (Dropout)               (None, 768)          0           lambda_3[0][0]
__________________________________________________________________________________________________
dense_7 (Dense)                 (None, 2)            1538        dropout[0][0]
==================================================================================================
Total params: 108,899,666
Trainable params: 108,899,666
Non-trainable params: 0
__________________________________________________________________________________________________
apply fgm
start to fitting...
Epoch 1/10
30/31 [============================>.] - ETA: 23s - loss: 1.2200- val_acc 0.46875 - val_f1 0.4301012687696427
new best model, save model to  save\clf_model.weights...
31/31 [==============================] - 774s 25s/step - loss: 1.2126
Epoch 2/10
30/31 [============================>.] - ETA: 29s - loss: 0.9212- val_acc 0.65625 - val_f1 0.653846153846154
__________________________________________________________________________________________________
dense_7 (Dense)                 (None, 2)            1538        dropout[0][0]
==================================================================================================
Total params: 108,899,666
Trainable params: 108,899,666
Non-trainable params: 0
__________________________________________________________________________________________________
apply fgm
start to fitting...
Epoch 1/10
30/31 [============================>.] - ETA: 23s - loss: 1.2200- val_acc 0.46875 - val_f1 0.4301012687696427
new best model, save model to  save\clf_model.weights...
31/31 [==============================] - 774s 25s/step - loss: 1.2126
Epoch 2/10
30/31 [============================>.] - ETA: 29s - loss: 0.9212- val_acc 0.65625 - val_f1 0.653846153846154
Trainable params: 108,899,666
Non-trainable params: 0
__________________________________________________________________________________________________
apply fgm
start to fitting...
Epoch 1/10
30/31 [============================>.] - ETA: 23s - loss: 1.2200- val_acc 0.46875 - val_f1 0.4301012687696427
new best model, save model to  save\clf_model.weights...
31/31 [==============================] - 774s 25s/step - loss: 1.2126
Epoch 2/10
30/31 [============================>.] - ETA: 29s - loss: 0.9212- val_acc 0.65625 - val_f1 0.653846153846154
apply fgm
start to fitting...
Epoch 1/10
30/31 [============================>.] - ETA: 23s - loss: 1.2200- val_acc 0.46875 - val_f1 0.4301012687696427
new best model, save model to  save\clf_model.weights...
31/31 [==============================] - 774s 25s/step - loss: 1.2126
Epoch 2/10
30/31 [============================>.] - ETA: 29s - loss: 0.9212- val_acc 0.65625 - val_f1 0.653846153846154
Epoch 1/10
30/31 [============================>.] - ETA: 23s - loss: 1.2200- val_acc 0.46875 - val_f1 0.4301012687696427
new best model, save model to  save\clf_model.weights...
31/31 [==============================] - 774s 25s/step - loss: 1.2126
Epoch 2/10
30/31 [============================>.] - ETA: 29s - loss: 0.9212- val_acc 0.65625 - val_f1 0.653846153846154
new best model, save model to  save\clf_model.weights...
31/31 [==============================] - 945s 30s/step - loss: 0.9213
Epoch 3/10
30/31 [============================>.] - ETA: 28s - loss: 0.8804- val_acc 0.625 - val_f1 0.6223776223776225
31/31 [==============================] - 904s 29s/step - loss: 0.8797
31/31 [==============================] - 774s 25s/step - loss: 1.2126
Epoch 2/10
30/31 [============================>.] - ETA: 29s - loss: 0.9212- val_acc 0.65625 - val_f1 0.653846153846154
new best model, save model to  save\clf_model.weights...
31/31 [==============================] - 945s 30s/step - loss: 0.9213
Epoch 3/10
30/31 [============================>.] - ETA: 28s - loss: 0.8804- val_acc 0.625 - val_f1 0.6223776223776225
31/31 [==============================] - 904s 29s/step - loss: 0.8797
new best model, save model to  save\clf_model.weights...
31/31 [==============================] - 945s 30s/step - loss: 0.9213
Epoch 3/10
30/31 [============================>.] - ETA: 28s - loss: 0.8804- val_acc 0.625 - val_f1 0.6223776223776225
31/31 [==============================] - 904s 29s/step - loss: 0.8797
Epoch 4/10
31/31 [==============================] - 904s 29s/step - loss: 0.8797
Epoch 4/10
Epoch 4/10
30/31 [============================>.] - ETA: 27s - loss: 0.8298- val_acc 0.640625 - val_f1 0.6384180790960452
31/31 [==============================] - 892s 29s/step - loss: 0.8275
Epoch 5/10
30/31 [============================>.] - ETA: 30s - loss: 0.8098 - val_acc 0.6770833333333334 - val_f1 0.676943117672601
new best model, save model to  save\clf_model.weights...
31/31 [==============================] - 984s 32s/step - loss: 0.8074
Epoch 6/10
30/31 [============================>.] - ETA: 29s - loss: 0.7912- val_acc 0.6770833333333334 - val_f1 0.6710511771858074
31/31 [==============================] - 957s 31s/step - loss: 0.7903
Epoch 7/10
30/31 [============================>.] - ETA: 28s - loss: 0.7444- val_acc 0.6979166666666666 - val_f1 0.6978838849701574
new best model, save model to  save\clf_model.weights...
31/31 [==============================] - 912s 29s/step - loss: 0.7441
Epoch 8/10
30/31 [============================>.] - ETA: 27s - loss: 0.7527- val_acc 0.7083333333333334 - val_f1 0.7080482241772564
new best model, save model to  save\clf_model.weights...
31/31 [==============================] - 885s 29s/step - loss: 0.7540
Epoch 9/10
30/31 [============================>.] - ETA: 29s - loss: 0.7298- val_acc 0.6822916666666666 - val_f1 0.6803406206501269
31/31 [==============================] - 940s 30s/step - loss: 0.7297
Epoch 10/10
30/31 [============================>.] - ETA: 29s - loss: 0.7249- val_acc 0.6927083333333334 - val_f1 0.6924997964114119
31/31 [==============================] - 947s 31s/step - loss: 0.7257
iteration 1 accuracy: 0.7083333333333334, f1: 0.7080482241772564

Average accuracy: 0.7083333333333334
Average f1: 0.7080482241772564