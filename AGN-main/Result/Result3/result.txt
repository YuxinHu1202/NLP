(text2) C:\Users\Lenovo\Desktop\大三上\自然语言处理\跑模型\AGN-main>python main.py sst2.json
2024-12-11 12:05:49.025412: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_100.dll'; dlerror: cudart64_100.dll not found
2024-12-11 12:05:49.025532: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
Using TensorFlow backend.
config:
{'ae_epochs': 10,
 'batch_size': 32,
 'dev_path': 'SST2/test.jsonl',
 'dropout': 0.5,
 'epochs': 3,
 'epsilon': 0.05,
 'fgm_epsilon': 0.3,
 'iterations': 1,
 'learning_rate': 3e-05,
 'max_len': 40,
 'pretrained_model_dir': 'uncased_L-12_H-768_A-12',
 'save_dir': 'save3',
 'train_path': 'SST2/train.jsonl',
 'verbose': 1}
successful!
load data...
batch alignment...
previous data size: 10000
alignment data size: 9984
set tcol....
token size: 13805
done to set tcol...
train vae...
WARN:tensorflow:From D:\anaconda3\envs\text2\lib\site-packages\tensorflow_core\python\ops\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
WARN:tensorflow:From D:\anaconda3\envs\text2\lib\site-packages\tensorflow_core\python\ops\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
train size: 7008
dev size: 2976
2024-12-11 12:05:54.400427: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
2024-12-11 12:05:55.488450: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: NVIDIA GeForce MX450 major: 7 minor: 5 memoryClockRate(GHz): 1.575
pciBusID: 0000:01:00.0
2024-12-11 12:05:55.492354: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_100.dll'; dlerror: cudart64_100.dll not found
2024-12-11 12:05:55.495774: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cublas64_100.dll'; dlerror: cublas64_100.dll not found
2024-12-11 12:05:55.499991: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cufft64_100.dll'; dlerror: cufft64_100.dll not found
2024-12-11 12:05:55.504513: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'curand64_100.dll'; dlerror: curand64_100.dll not found
2024-12-11 12:05:55.508690: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cusolver64_100.dll'; dlerror: cusolver64_100.dll not found
2024-12-11 12:05:55.512263: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cusparse64_100.dll'; dlerror: cusparse64_100.dll not found
2024-12-11 12:05:55.516242: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudnn64_7.dll'; dlerror: cudnn64_7.dll not found
2024-12-11 12:05:55.516415: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1641] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2024-12-11 12:05:55.517028: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2024-12-11 12:05:55.519434: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2024-12-11 12:05:55.519580: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]
WARN:tensorflow:From D:\anaconda3\envs\text2\lib\site-packages\keras\backend\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

Train on 7008 samples, validate on 2976 samples
Epoch 1/10
 - 0s - loss: 0.6764 - val_loss: 0.6100
Epoch 2/10
 - 0s - loss: 0.5925 - val_loss: 0.5843
Epoch 3/10
 - 0s - loss: 0.5766 - val_loss: 0.5743
Epoch 4/10
 - 0s - loss: 0.5699 - val_loss: 0.5702
Epoch 5/10
 - 0s - loss: 0.5664 - val_loss: 0.5657
Epoch 6/10
 - 0s - loss: 0.5624 - val_loss: 0.5626
Epoch 7/10
 - 0s - loss: 0.5600 - val_loss: 0.5586
Epoch 8/10
 - 0s - loss: 0.5554 - val_loss: 0.5559
Epoch 9/10
 - 0s - loss: 0.5516 - val_loss: 0.5537
Epoch 10/10
 - 0s - loss: 0.5488 - val_loss: 0.5516
batch alignment...
previous data size: 2090
alignment data size: 2080
train vae...
build generator
Traceback (most recent call last):
  File "main.py", line 110, in <module>
    model = AGNClassifier(config)
  File "C:\Users\Lenovo\Desktop\大三上\自然语言处理\跑模型\AGN-main\model.py", line 226, in __init__
    self.build()
  File "C:\Users\Lenovo\Desktop\大三上\自然语言处理\跑模型\AGN-main\model.py", line 233, in build
    'bert_model.ckpt'),
  File "D:\anaconda3\envs\text2\lib\site-packages\langml\plm\bert.py", line 342, in load_bert
    model = restore(model)
  File "D:\anaconda3\envs\text2\lib\site-packages\langml\plm\bert.py", line 268, in restore
    variables('bert/embeddings/word_embeddings'),
  File "D:\anaconda3\envs\text2\lib\site-packages\keras\engine\base_layer.py", line 1120, in set_weights
    param_values = K.batch_get_value(params)
  File "D:\anaconda3\envs\text2\lib\site-packages\keras\backend\tensorflow_backend.py", line 2939, in batch_get_value
    return tf_keras_backend.batch_get_value(ops)
  File "D:\anaconda3\envs\text2\lib\site-packages\tensorflow_core\python\keras\backend.py", line 3185, in batch_get_value
    return get_session(tensors).run(tensors)
  File "D:\anaconda3\envs\text2\lib\site-packages\tensorflow_core\python\keras\backend.py", line 486, in get_session
    _initialize_variables(session)
  File "D:\anaconda3\envs\text2\lib\site-packages\tensorflow_core\python\keras\backend.py", line 903, in _initialize_variables
    [variables_module.is_variable_initialized(v) for v in candidate_vars])
  File "D:\anaconda3\envs\text2\lib\site-packages\tensorflow_core\python\client\session.py", line 956, in run
    run_metadata_ptr)
  File "D:\anaconda3\envs\text2\lib\site-packages\tensorflow_core\python\client\session.py", line 1180, in _run
    feed_dict_tensor, options, run_metadata)
  File "D:\anaconda3\envs\text2\lib\site-packages\tensorflow_core\python\client\session.py", line 1359, in _do_run
    run_metadata)
  File "D:\anaconda3\envs\text2\lib\site-packages\tensorflow_core\python\client\session.py", line 1365, in _do_call
    return fn(*args)
  File "D:\anaconda3\envs\text2\lib\site-packages\tensorflow_core\python\client\session.py", line 1348, in _run_fn
    self._extend_graph()
  File "D:\anaconda3\envs\text2\lib\site-packages\tensorflow_core\python\client\session.py", line 1388, in _extend_graph
    tf_session.ExtendSession(self._session)
KeyboardInterrupt

(text2) C:\Users\Lenovo\Desktop\大三上\自然语言处理\跑模型\AGN-main>python main.py sst2.json
2024-12-11 12:06:11.226079: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_100.dll'; dlerror: cudart64_100.dll not found
2024-12-11 12:06:11.226348: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
Using TensorFlow backend.
config:
{'ae_epochs': 100,
 'batch_size': 32,
 'dev_path': 'SST2/test.jsonl',
 'dropout': 0.5,
 'epochs': 10,
 'epsilon': 0.05,
 'fgm_epsilon': 0.3,
 'iterations': 1,
 'learning_rate': 3e-05,
 'max_len': 40,
 'pretrained_model_dir': 'uncased_L-12_H-768_A-12',
 'save_dir': 'save3',
 'train_path': 'SST2/train.jsonl',
 'verbose': 1}
successful!
load data...
batch alignment...
previous data size: 10000
alignment data size: 9984
set tcol....
token size: 13805
done to set tcol...
train vae...
WARN:tensorflow:From D:\anaconda3\envs\text2\lib\site-packages\tensorflow_core\python\ops\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
WARN:tensorflow:From D:\anaconda3\envs\text2\lib\site-packages\tensorflow_core\python\ops\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
train size: 7008
dev size: 2976
2024-12-11 12:06:15.533455: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
2024-12-11 12:06:16.476893: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: NVIDIA GeForce MX450 major: 7 minor: 5 memoryClockRate(GHz): 1.575
pciBusID: 0000:01:00.0
2024-12-11 12:06:16.481096: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_100.dll'; dlerror: cudart64_100.dll not found
2024-12-11 12:06:16.484026: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cublas64_100.dll'; dlerror: cublas64_100.dll not found
2024-12-11 12:06:16.487481: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cufft64_100.dll'; dlerror: cufft64_100.dll not found
2024-12-11 12:06:16.491027: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'curand64_100.dll'; dlerror: curand64_100.dll not found
2024-12-11 12:06:16.495204: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cusolver64_100.dll'; dlerror: cusolver64_100.dll not found
2024-12-11 12:06:16.499608: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cusparse64_100.dll'; dlerror: cusparse64_100.dll not found
2024-12-11 12:06:16.503804: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudnn64_7.dll'; dlerror: cudnn64_7.dll not found
2024-12-11 12:06:16.504022: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1641] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2024-12-11 12:06:16.504755: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2024-12-11 12:06:16.513705: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2024-12-11 12:06:16.514020: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]
WARN:tensorflow:From D:\anaconda3\envs\text2\lib\site-packages\keras\backend\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

Train on 7008 samples, validate on 2976 samples
Epoch 1/100
 - 0s - loss: 0.6734 - val_loss: 0.6085
Epoch 2/100
 - 0s - loss: 0.5928 - val_loss: 0.5831
Epoch 3/100
 - 0s - loss: 0.5773 - val_loss: 0.5741
Epoch 4/100
 - 0s - loss: 0.5698 - val_loss: 0.5710
Epoch 5/100
 - 0s - loss: 0.5660 - val_loss: 0.5637
Epoch 6/100
 - 0s - loss: 0.5625 - val_loss: 0.5616
Epoch 7/100
 - 0s - loss: 0.5583 - val_loss: 0.5594
Epoch 8/100
 - 0s - loss: 0.5551 - val_loss: 0.5545
Epoch 9/100
 - 0s - loss: 0.5520 - val_loss: 0.5520
Epoch 10/100
 - 0s - loss: 0.5507 - val_loss: 0.5498
Epoch 11/100
 - 0s - loss: 0.5462 - val_loss: 0.5476
Epoch 12/100
 - 0s - loss: 0.5436 - val_loss: 0.5439
Epoch 13/100
 - 0s - loss: 0.5408 - val_loss: 0.5423
Epoch 14/100
 - 0s - loss: 0.5384 - val_loss: 0.5406
Epoch 15/100
 - 0s - loss: 0.5368 - val_loss: 0.5362
Epoch 16/100
 - 0s - loss: 0.5333 - val_loss: 0.5333
Epoch 17/100
 - 0s - loss: 0.5304 - val_loss: 0.5303
Epoch 18/100
 - 0s - loss: 0.5273 - val_loss: 0.5280
Epoch 19/100
 - 0s - loss: 0.5246 - val_loss: 0.5248
Epoch 20/100
 - 0s - loss: 0.5235 - val_loss: 0.5224
Epoch 21/100
 - 0s - loss: 0.5218 - val_loss: 0.5229
Epoch 22/100
 - 0s - loss: 0.5203 - val_loss: 0.5212
Epoch 23/100
 - 0s - loss: 0.5194 - val_loss: 0.5210
Epoch 24/100
 - 0s - loss: 0.5189 - val_loss: 0.5195
Epoch 25/100
 - 0s - loss: 0.5177 - val_loss: 0.5187
Epoch 26/100
 - 0s - loss: 0.5172 - val_loss: 0.5184
Epoch 27/100
 - 0s - loss: 0.5176 - val_loss: 0.5189
Epoch 28/100
 - 0s - loss: 0.5170 - val_loss: 0.5185
Epoch 29/100
 - 0s - loss: 0.5162 - val_loss: 0.5182
Epoch 30/100
 - 0s - loss: 0.5163 - val_loss: 0.5175
Epoch 31/100
 - 0s - loss: 0.5164 - val_loss: 0.5174
Epoch 32/100
 - 0s - loss: 0.5160 - val_loss: 0.5167
Epoch 33/100
 - 0s - loss: 0.5154 - val_loss: 0.5161
Epoch 34/100
 - 0s - loss: 0.5149 - val_loss: 0.5170
Epoch 35/100
 - 0s - loss: 0.5154 - val_loss: 0.5166
Epoch 36/100
 - 0s - loss: 0.5151 - val_loss: 0.5169
Epoch 37/100
 - 0s - loss: 0.5153 - val_loss: 0.5158
Epoch 38/100
 - 0s - loss: 0.5147 - val_loss: 0.5169
Epoch 39/100
 - 0s - loss: 0.5150 - val_loss: 0.5155
Epoch 40/100
 - 0s - loss: 0.5144 - val_loss: 0.5160
Epoch 41/100
 - 0s - loss: 0.5145 - val_loss: 0.5165
Epoch 42/100
 - 0s - loss: 0.5148 - val_loss: 0.5168
Epoch 43/100
 - 0s - loss: 0.5144 - val_loss: 0.5157
Epoch 44/100
 - 0s - loss: 0.5144 - val_loss: 0.5156
batch alignment...
previous data size: 2090
 - 0s - loss: 0.5154 - val_loss: 0.5161
Epoch 34/100
 - 0s - loss: 0.5149 - val_loss: 0.5170
Epoch 35/100
 - 0s - loss: 0.5154 - val_loss: 0.5166
Epoch 36/100
 - 0s - loss: 0.5151 - val_loss: 0.5169
Epoch 37/100
 - 0s - loss: 0.5153 - val_loss: 0.5158
Epoch 38/100
 - 0s - loss: 0.5147 - val_loss: 0.5169
Epoch 39/100
 - 0s - loss: 0.5150 - val_loss: 0.5155
Epoch 40/100
 - 0s - loss: 0.5144 - val_loss: 0.5160
Epoch 41/100
 - 0s - loss: 0.5145 - val_loss: 0.5165
Epoch 42/100
 - 0s - loss: 0.5148 - val_loss: 0.5168
Epoch 43/100
 - 0s - loss: 0.5144 - val_loss: 0.5157
Epoch 44/100
 - 0s - loss: 0.5144 - val_loss: 0.5156
batch alignment...
previous data size: 2090
 - 0s - loss: 0.5153 - val_loss: 0.5158
Epoch 38/100
 - 0s - loss: 0.5147 - val_loss: 0.5169
Epoch 39/100
 - 0s - loss: 0.5150 - val_loss: 0.5155
Epoch 40/100
 - 0s - loss: 0.5144 - val_loss: 0.5160
Epoch 41/100
 - 0s - loss: 0.5145 - val_loss: 0.5165
Epoch 42/100
 - 0s - loss: 0.5148 - val_loss: 0.5168
Epoch 43/100
 - 0s - loss: 0.5144 - val_loss: 0.5157
Epoch 44/100
 - 0s - loss: 0.5144 - val_loss: 0.5156
batch alignment...
previous data size: 2090
Epoch 40/100
 - 0s - loss: 0.5144 - val_loss: 0.5160
Epoch 41/100
 - 0s - loss: 0.5145 - val_loss: 0.5165
Epoch 42/100
 - 0s - loss: 0.5148 - val_loss: 0.5168
Epoch 43/100
 - 0s - loss: 0.5144 - val_loss: 0.5157
Epoch 44/100
 - 0s - loss: 0.5144 - val_loss: 0.5156
batch alignment...
previous data size: 2090
 - 0s - loss: 0.5144 - val_loss: 0.5157
Epoch 44/100
 - 0s - loss: 0.5144 - val_loss: 0.5156
batch alignment...
previous data size: 2090
 - 0s - loss: 0.5144 - val_loss: 0.5156
batch alignment...
previous data size: 2090
previous data size: 2090
alignment data size: 2080
train vae...
build generator
Skip Embedding-Mapping
Model: "model_4"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
Input-Token (InputLayer)        (None, None)         0
__________________________________________________________________________________________________
Input-Segment (InputLayer)      (None, None)         0
__________________________________________________________________________________________________
Embedding-Token (TokenEmbedding [(None, None, 768),  23440896    Input-Token[0][0]
__________________________________________________________________________________________________
Embedding-Segment (Embedding)   (None, None, 768)    1536        Input-Segment[0][0]
__________________________________________________________________________________________________
Embedding-Token-Segment (Add)   (None, None, 768)    0           Embedding-Token[0][0]
                                                                 Embedding-Segment[0][0]
__________________________________________________________________________________________________
Embedding-Position (AbsolutePos (None, None, 768)    393216      Embedding-Token-Segment[0][0]    
__________________________________________________________________________________________________
Embedding-Norm (LayerNorm)      (None, None, 768)    1536        Embedding-Position[0][0]
__________________________________________________________________________________________________
Embedding-Dropout (Dropout)     (None, None, 768)    0           Embedding-Norm[0][0]
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    2362368     Embedding-Dropout[0][0]
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    0           Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    0           Embedding-Dropout[0][0]
                                                                 Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-FeedForward (Feed (None, None, 768)    4722432     Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-FeedForward-Dropo (None, None, 768)    0           Transformer-1-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-1-FeedForward-Add ( (None, None, 768)    0           Transformer-1-MultiHeadSelfAttent
                                                                 Transformer-1-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-1-FeedForward-Norm  (None, None, 768)    1536        Transformer-1-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-1-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    0           Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    0           Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-FeedForward (Feed (None, None, 768)    4722432     Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-FeedForward-Dropo (None, None, 768)    0           Transformer-2-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-2-FeedForward-Add ( (None, None, 768)    0           Transformer-2-MultiHeadSelfAttent
                                                                 Transformer-2-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-2-FeedForward-Norm  (None, None, 768)    1536        Transformer-2-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-2-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    0           Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    0           Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-FeedForward (Feed (None, None, 768)    4722432     Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-FeedForward-Dropo (None, None, 768)    0           Transformer-3-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-3-FeedForward-Add ( (None, None, 768)    0           Transformer-3-MultiHeadSelfAttent
                                                                 Transformer-3-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-3-FeedForward-Norm  (None, None, 768)    1536        Transformer-3-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-3-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    0           Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    0           Transformer-3-FeedForward-Norm[0]
                                                                 Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-FeedForward (Feed (None, None, 768)    4722432     Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-FeedForward-Dropo (None, None, 768)    0           Transformer-4-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-4-FeedForward-Add ( (None, None, 768)    0           Transformer-4-MultiHeadSelfAttent
                                                                 Transformer-4-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-4-FeedForward-Norm  (None, None, 768)    1536        Transformer-4-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-4-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    0           Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    0           Transformer-4-FeedForward-Norm[0]
                                                                 Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-FeedForward (Feed (None, None, 768)    4722432     Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-FeedForward-Dropo (None, None, 768)    0           Transformer-5-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-5-FeedForward-Add ( (None, None, 768)    0           Transformer-5-MultiHeadSelfAttent
                                                                 Transformer-5-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-5-FeedForward-Norm  (None, None, 768)    1536        Transformer-5-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-5-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 768)    0           Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 768)    0           Transformer-5-FeedForward-Norm[0]
                                                                 Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-FeedForward (Feed (None, None, 768)    4722432     Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-FeedForward-Dropo (None, None, 768)    0           Transformer-6-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-6-FeedForward-Add ( (None, None, 768)    0           Transformer-6-MultiHeadSelfAttent
                                                                 Transformer-6-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-6-FeedForward-Norm  (None, None, 768)    1536        Transformer-6-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-6-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 768)    0           Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 768)    0           Transformer-6-FeedForward-Norm[0]
                                                                 Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-FeedForward (Feed (None, None, 768)    4722432     Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-FeedForward-Dropo (None, None, 768)    0           Transformer-7-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-7-FeedForward-Add ( (None, None, 768)    0           Transformer-7-MultiHeadSelfAttent
                                                                 Transformer-7-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-7-FeedForward-Norm  (None, None, 768)    1536        Transformer-7-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-7-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 768)    0           Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 768)    0           Transformer-7-FeedForward-Norm[0]
                                                                 Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-FeedForward (Feed (None, None, 768)    4722432     Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-FeedForward-Dropo (None, None, 768)    0           Transformer-8-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-8-FeedForward-Add ( (None, None, 768)    0           Transformer-8-MultiHeadSelfAttent
                                                                 Transformer-8-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-8-FeedForward-Norm  (None, None, 768)    1536        Transformer-8-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-8-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 768)    0           Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 768)    0           Transformer-8-FeedForward-Norm[0]
                                                                 Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-FeedForward (Feed (None, None, 768)    4722432     Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-FeedForward-Dropo (None, None, 768)    0           Transformer-9-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-9-FeedForward-Add ( (None, None, 768)    0           Transformer-9-MultiHeadSelfAttent
                                                                 Transformer-9-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-9-FeedForward-Norm  (None, None, 768)    1536        Transformer-9-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 768)    2362368     Transformer-9-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 768)    0           Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 768)    0           Transformer-9-FeedForward-Norm[0]
                                                                 Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 768)    1536        Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-FeedForward (Fee (None, None, 768)    4722432     Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-FeedForward-Drop (None, None, 768)    0           Transformer-10-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-10-FeedForward-Add  (None, None, 768)    0           Transformer-10-MultiHeadSelfAtten
                                                                 Transformer-10-FeedForward-Dropou
__________________________________________________________________________________________________
Transformer-10-FeedForward-Norm (None, None, 768)    1536        Transformer-10-FeedForward-Add[0]
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 768)    2362368     Transformer-10-FeedForward-Norm[0
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 768)    0           Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 768)    0           Transformer-10-FeedForward-Norm[0
                                                                 Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 768)    1536        Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-FeedForward (Fee (None, None, 768)    4722432     Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-FeedForward-Drop (None, None, 768)    0           Transformer-11-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-11-FeedForward-Add  (None, None, 768)    0           Transformer-11-MultiHeadSelfAtten
                                                                 Transformer-11-FeedForward-Dropou
__________________________________________________________________________________________________
Transformer-11-FeedForward-Norm (None, None, 768)    1536        Transformer-11-FeedForward-Add[0]
__________________________________________________________________________________________________
Transformer-12-MultiHeadSelfAtt (None, None, 768)    2362368     Transformer-11-FeedForward-Norm[0
__________________________________________________________________________________________________
Transformer-12-MultiHeadSelfAtt (None, None, 768)    0           Transformer-12-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-12-MultiHeadSelfAtt (None, None, 768)    0           Transformer-11-FeedForward-Norm[0
                                                                 Transformer-12-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-12-MultiHeadSelfAtt (None, None, 768)    1536        Transformer-12-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-12-FeedForward (Fee (None, None, 768)    4722432     Transformer-12-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-12-FeedForward-Drop (None, None, 768)    0           Transformer-12-FeedForward[0][0]
__________________________________________________________________________________________________
gi (InputLayer)                 (None, 40)           0
__________________________________________________________________________________________________
Transformer-12-FeedForward-Add  (None, None, 768)    0           Transformer-12-MultiHeadSelfAtten
                                                                 Transformer-12-FeedForward-Dropou
__________________________________________________________________________________________________
dense_5 (Dense)                 (None, 40)           1640        gi[0][0]
__________________________________________________________________________________________________
Transformer-12-FeedForward-Norm (None, None, 768)    1536        Transformer-12-FeedForward-Add[0]
__________________________________________________________________________________________________
lambda_2 (Lambda)               (None, 40, 1)        0           dense_5[0][0]
__________________________________________________________________________________________________
agn_1 (AGN)                     [(None, None, 768),  0           Transformer-12-FeedForward-Norm[0
                                                                 lambda_2[0][0]
__________________________________________________________________________________________________
lambda_1 (Lambda)               (None, None, 1)      0           Input-Token[0][0]
__________________________________________________________________________________________________
lambda_3 (Lambda)               (None, None, 768)    0           agn_1[0][0]
                                                                 lambda_1[0][0]
__________________________________________________________________________________________________
lambda_4 (Lambda)               (None, 768)          0           lambda_3[0][0]
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 768)          0           lambda_4[0][0]
__________________________________________________________________________________________________
dense_8 (Dense)                 (None, 2)            1538        dropout_1[0][0]
==================================================================================================
Total params: 108,894,826
Trainable params: 108,894,826
Non-trainable params: 0
__________________________________________________________________________________________________
apply fgm
start to fitting...
Epoch 1/10
312/312 [==============================] - 3527s 11s/step - loss: 0.8819
- val_acc 0.5432692307692307 - val_f1 0.5259112532737555
new best model, save model to  save3\clf_model.weights...
Epoch 2/10
312/312 [==============================] - 4056s 13s/step - loss: 0.7751
Non-trainable params: 0
__________________________________________________________________________________________________
apply fgm
start to fitting...
Epoch 1/10
312/312 [==============================] - 3527s 11s/step - loss: 0.8819
- val_acc 0.5432692307692307 - val_f1 0.5259112532737555
new best model, save model to  save3\clf_model.weights...
Epoch 2/10
312/312 [==============================] - 4056s 13s/step - loss: 0.7751
start to fitting...
Epoch 1/10
312/312 [==============================] - 3527s 11s/step - loss: 0.8819
- val_acc 0.5432692307692307 - val_f1 0.5259112532737555
new best model, save model to  save3\clf_model.weights...
Epoch 2/10
312/312 [==============================] - 4056s 13s/step - loss: 0.7751
new best model, save model to  save3\clf_model.weights...
Epoch 2/10
312/312 [==============================] - 4056s 13s/step - loss: 0.7751
312/312 [==============================] - 4056s 13s/step - loss: 0.7751
- val_acc 0.5480769230769231 - val_f1 0.5480062990611514
new best model, save model to  save3\clf_model.weights...
Epoch 3/10
312/312 [==============================] - 4134s 13s/step - loss: 0.7623
- val_acc 0.5850961538461539 - val_f1 0.5850039730071329
new best model, save model to  save3\clf_model.weights...
Epoch 4/10
312/312 [==============================] - 4133s 13s/step - loss: 0.7409
- val_acc 0.5793269230769231 - val_f1 0.5504697714385782
Epoch 5/10
312/312 [==============================] - 4142s 13s/step - loss: 0.7275
- val_acc 0.6100961538461539 - val_f1 0.610093900782988
new best model, save model to  save3\clf_model.weights...
Epoch 6/10
312/312 [==============================] - 4463s 14s/step - loss: 0.7190
- val_acc 0.6163461538461539 - val_f1 0.6138868961050925
new best model, save model to  save3\clf_model.weights...
Epoch 7/10
312/312 [==============================] - 4770s 15s/step - loss: 0.7029
- val_acc 0.6235576923076923 - val_f1 0.6215632543831338
new best model, save model to  save3\clf_model.weights...
Epoch 8/10
312/312 [==============================] - 4659s 15s/step - loss: 0.6974
- val_acc 0.6370192307692307 - val_f1 0.6286092879368257
new best model, save model to  save3\clf_model.weights...
Epoch 9/10
312/312 [==============================] - 4178s 13s/step - loss: 0.6885
- val_acc 0.6370192307692307 - val_f1 0.6361269579551003
new best model, save model to  save3\clf_model.weights...
Epoch 10/10
312/312 [==============================] - 4018s 13s/step - loss: 0.6848
- val_acc 0.6350961538461538 - val_f1 0.6349401359817469
iteration 1 accuracy: 0.6370192307692307, f1: 0.6361269579551003

Average accuracy: 0.6370192307692307
Average f1: 0.6361269579551003