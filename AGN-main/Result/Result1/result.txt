(text2) C:\Users\Lenovo\Desktop\大三上\自然语言处理\跑模型\AGN-main>python main.py sst2.json
2024-12-10 01:46:46.685812: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_100.dll'; dlerror: cudart64_100.dll not found
2024-12-10 01:46:46.686184: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
Using TensorFlow backend.
config:
{'ae_epochs': 100,
 'batch_size': 32,
 'dev_path': 'SST2/test.jsonl',
 'dropout': 0.3,
 'epochs': 10,
 'epsilon': 0.05,
 'fgm_epsilon': 0.3,
 'iterations': 1,
 'learning_rate': 3e-05,
 'max_len': 80,
 'pretrained_model_dir': 'uncased_L-12_H-768_A-12',
 'save_dir': 'save',
 'train_path': 'SST2/train.jsonl',
 'verbose': 1}
successful!
load data...
batch alignment...
previous data size: 201
alignment data size: 192
set tcol....
token size: 1567
done to set tcol...
train vae...
WARN:tensorflow:From D:\anaconda3\envs\text2\lib\site-packages\tensorflow_core\python\ops\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
WARN:tensorflow:From D:\anaconda3\envs\text2\lib\site-packages\tensorflow_core\python\ops\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
train size: 160
dev size: 32
2024-12-10 01:46:51.305558: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
2024-12-10 01:46:51.339699: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: NVIDIA GeForce MX450 major: 7 minor: 5 memoryClockRate(GHz): 1.575
pciBusID: 0000:01:00.0
2024-12-10 01:46:51.345191: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_100.dll'; dlerror: cudart64_100.dll not found
2024-12-10 01:46:51.351183: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cublas64_100.dll'; dlerror: cublas64_100.dll not found
2024-12-10 01:46:51.357324: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cufft64_100.dll'; dlerror: cufft64_100.dll not found
2024-12-10 01:46:51.362623: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'curand64_100.dll'; dlerror: curand64_100.dll not found
2024-12-10 01:46:51.367529: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cusolver64_100.dll'; dlerror: cusolver64_100.dll not found
2024-12-10 01:46:51.373158: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cusparse64_100.dll'; dlerror: cusparse64_100.dll not found
2024-12-10 01:46:51.379704: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudnn64_7.dll'; dlerror: cudnn64_7.dll not found
2024-12-10 01:46:51.380065: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1641] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2024-12-10 01:46:51.380882: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2024-12-10 01:46:51.384789: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2024-12-10 01:46:51.385051: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]
WARN:tensorflow:From D:\anaconda3\envs\text2\lib\site-packages\keras\backend\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

Train on 160 samples, validate on 32 samples
Epoch 1/100
 - 1s - loss: 0.6932 - val_loss: 0.6893
Epoch 2/100
 - 0s - loss: 0.6898 - val_loss: 0.6817
Epoch 3/100
 - 0s - loss: 0.6836 - val_loss: 0.6817
Epoch 4/100
 - 0s - loss: 0.6763 - val_loss: 0.6597
Epoch 5/100
 - 0s - loss: 0.6579 - val_loss: 0.6328
Epoch 6/100
 - 0s - loss: 0.6318 - val_loss: 0.6031
Epoch 7/100
 - 0s - loss: 0.5863 - val_loss: 0.5555
Epoch 8/100
 - 0s - loss: 0.5357 - val_loss: 0.5235
Epoch 9/100
 - 0s - loss: 0.4920 - val_loss: 0.4456
Epoch 10/100
 - 0s - loss: 0.4385 - val_loss: 0.4094
Epoch 11/100
 - 0s - loss: 0.3926 - val_loss: 0.3812
Epoch 12/100
 - 0s - loss: 0.3583 - val_loss: 0.3344
Epoch 13/100
 - 0s - loss: 0.3398 - val_loss: 0.3188
Epoch 14/100
 - 0s - loss: 0.3094 - val_loss: 0.2973
Epoch 15/100
 - 0s - loss: 0.3004 - val_loss: 0.2744
Epoch 16/100
 - 0s - loss: 0.2838 - val_loss: 0.2599
Epoch 17/100
 - 0s - loss: 0.2513 - val_loss: 0.2562
Epoch 18/100
 - 0s - loss: 0.2526 - val_loss: 0.2584
Epoch 19/100
 - 0s - loss: 0.2462 - val_loss: 0.2383
Epoch 20/100
 - 0s - loss: 0.2356 - val_loss: 0.2193
Epoch 21/100
 - 0s - loss: 0.2304 - val_loss: 0.2261
Epoch 22/100
 - 0s - loss: 0.2204 - val_loss: 0.2159
Epoch 23/100
 - 0s - loss: 0.2144 - val_loss: 0.2074
Epoch 24/100
 - 0s - loss: 0.1990 - val_loss: 0.2149
Epoch 25/100
 - 0s - loss: 0.2002 - val_loss: 0.1924
Epoch 26/100
 - 0s - loss: 0.1981 - val_loss: 0.1946
Epoch 27/100
 - 0s - loss: 0.1829 - val_loss: 0.1940
Epoch 28/100
 - 0s - loss: 0.1968 - val_loss: 0.2088
Epoch 29/100
 - 0s - loss: 0.1884 - val_loss: 0.1892
Epoch 30/100
 - 0s - loss: 0.1836 - val_loss: 0.1828
Epoch 31/100
 - 0s - loss: 0.1825 - val_loss: 0.1801
Epoch 32/100
 - 0s - loss: 0.1717 - val_loss: 0.1781
Epoch 33/100
 - 0s - loss: 0.1781 - val_loss: 0.1796
Epoch 34/100
 - 0s - loss: 0.1721 - val_loss: 0.1519
Epoch 35/100
 - 0s - loss: 0.1736 - val_loss: 0.1603
Epoch 36/100
 - 0s - loss: 0.1656 - val_loss: 0.1603
Epoch 37/100
 - 0s - loss: 0.1594 - val_loss: 0.1680
Epoch 38/100
 - 0s - loss: 0.1578 - val_loss: 0.1596
Epoch 39/100
 - 0s - loss: 0.1532 - val_loss: 0.1552
batch alignment...
previous data size: 201
alignment data size: 192
train vae...
build generator
Skip Embedding-Mapping
Model: "model_4"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
Input-Token (InputLayer)        (None, None)         0
__________________________________________________________________________________________________
Input-Segment (InputLayer)      (None, None)         0
__________________________________________________________________________________________________
Embedding-Token (TokenEmbedding [(None, None, 768),  23440896    Input-Token[0][0]
__________________________________________________________________________________________________
Embedding-Segment (Embedding)   (None, None, 768)    1536        Input-Segment[0][0]
__________________________________________________________________________________________________
Embedding-Token-Segment (Add)   (None, None, 768)    0           Embedding-Token[0][0]
                                                                 Embedding-Segment[0][0]
__________________________________________________________________________________________________
Embedding-Position (AbsolutePos (None, None, 768)    393216      Embedding-Token-Segment[0][0]
__________________________________________________________________________________________________
Embedding-Norm (LayerNorm)      (None, None, 768)    1536        Embedding-Position[0][0]
__________________________________________________________________________________________________
Embedding-Dropout (Dropout)     (None, None, 768)    0           Embedding-Norm[0][0]
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    2362368     Embedding-Dropout[0][0]
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    0           Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    0           Embedding-Dropout[0][0]
                                                                 Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-FeedForward (Feed (None, None, 768)    4722432     Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-FeedForward-Dropo (None, None, 768)    0           Transformer-1-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-1-FeedForward-Add ( (None, None, 768)    0           Transformer-1-MultiHeadSelfAttent
                                                                 Transformer-1-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-1-FeedForward-Norm  (None, None, 768)    1536        Transformer-1-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-1-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    0           Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    0           Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-FeedForward (Feed (None, None, 768)    4722432     Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-FeedForward-Dropo (None, None, 768)    0           Transformer-2-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-2-FeedForward-Add ( (None, None, 768)    0           Transformer-2-MultiHeadSelfAttent
                                                                 Transformer-2-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-2-FeedForward-Norm  (None, None, 768)    1536        Transformer-2-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-2-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    0           Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    0           Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-FeedForward (Feed (None, None, 768)    4722432     Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-FeedForward-Dropo (None, None, 768)    0           Transformer-3-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-3-FeedForward-Add ( (None, None, 768)    0           Transformer-3-MultiHeadSelfAttent
                                                                 Transformer-3-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-3-FeedForward-Norm  (None, None, 768)    1536        Transformer-3-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-3-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    0           Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    0           Transformer-3-FeedForward-Norm[0]
                                                                 Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-FeedForward (Feed (None, None, 768)    4722432     Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-FeedForward-Dropo (None, None, 768)    0           Transformer-4-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-4-FeedForward-Add ( (None, None, 768)    0           Transformer-4-MultiHeadSelfAttent
                                                                 Transformer-4-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-4-FeedForward-Norm  (None, None, 768)    1536        Transformer-4-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-4-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    0           Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    0           Transformer-4-FeedForward-Norm[0]
                                                                 Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-FeedForward (Feed (None, None, 768)    4722432     Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-FeedForward-Dropo (None, None, 768)    0           Transformer-5-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-5-FeedForward-Add ( (None, None, 768)    0           Transformer-5-MultiHeadSelfAttent
                                                                 Transformer-5-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-5-FeedForward-Norm  (None, None, 768)    1536        Transformer-5-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-5-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 768)    0           Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 768)    0           Transformer-5-FeedForward-Norm[0]
                                                                 Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-FeedForward (Feed (None, None, 768)    4722432     Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-FeedForward-Dropo (None, None, 768)    0           Transformer-6-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-6-FeedForward-Add ( (None, None, 768)    0           Transformer-6-MultiHeadSelfAttent
                                                                 Transformer-6-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-6-FeedForward-Norm  (None, None, 768)    1536        Transformer-6-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-6-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 768)    0           Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 768)    0           Transformer-6-FeedForward-Norm[0]
                                                                 Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-FeedForward (Feed (None, None, 768)    4722432     Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-FeedForward-Dropo (None, None, 768)    0           Transformer-7-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-7-FeedForward-Add ( (None, None, 768)    0           Transformer-7-MultiHeadSelfAttent
                                                                 Transformer-7-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-7-FeedForward-Norm  (None, None, 768)    1536        Transformer-7-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-7-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 768)    0           Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 768)    0           Transformer-7-FeedForward-Norm[0]
                                                                 Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-FeedForward (Feed (None, None, 768)    4722432     Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-FeedForward-Dropo (None, None, 768)    0           Transformer-8-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-8-FeedForward-Add ( (None, None, 768)    0           Transformer-8-MultiHeadSelfAttent
                                                                 Transformer-8-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-8-FeedForward-Norm  (None, None, 768)    1536        Transformer-8-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-8-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 768)    0           Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 768)    0           Transformer-8-FeedForward-Norm[0]
                                                                 Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-FeedForward (Feed (None, None, 768)    4722432     Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-FeedForward-Dropo (None, None, 768)    0           Transformer-9-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-9-FeedForward-Add ( (None, None, 768)    0           Transformer-9-MultiHeadSelfAttent
                                                                 Transformer-9-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-9-FeedForward-Norm  (None, None, 768)    1536        Transformer-9-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 768)    2362368     Transformer-9-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 768)    0           Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 768)    0           Transformer-9-FeedForward-Norm[0]
                                                                 Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 768)    1536        Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-FeedForward (Fee (None, None, 768)    4722432     Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-FeedForward-Drop (None, None, 768)    0           Transformer-10-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-10-FeedForward-Add  (None, None, 768)    0           Transformer-10-MultiHeadSelfAtten
                                                                 Transformer-10-FeedForward-Dropou
__________________________________________________________________________________________________
Transformer-10-FeedForward-Norm (None, None, 768)    1536        Transformer-10-FeedForward-Add[0]
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 768)    2362368     Transformer-10-FeedForward-Norm[0
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 768)    0           Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 768)    0           Transformer-10-FeedForward-Norm[0
                                                                 Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 768)    1536        Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-FeedForward (Fee (None, None, 768)    4722432     Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-FeedForward-Drop (None, None, 768)    0           Transformer-11-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-11-FeedForward-Add  (None, None, 768)    0           Transformer-11-MultiHeadSelfAtten
                                                                 Transformer-11-FeedForward-Dropou
__________________________________________________________________________________________________
Transformer-11-FeedForward-Norm (None, None, 768)    1536        Transformer-11-FeedForward-Add[0]
__________________________________________________________________________________________________
Transformer-12-MultiHeadSelfAtt (None, None, 768)    2362368     Transformer-11-FeedForward-Norm[0
__________________________________________________________________________________________________
Transformer-12-MultiHeadSelfAtt (None, None, 768)    0           Transformer-12-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-12-MultiHeadSelfAtt (None, None, 768)    0           Transformer-11-FeedForward-Norm[0
                                                                 Transformer-12-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-12-MultiHeadSelfAtt (None, None, 768)    1536        Transformer-12-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-12-FeedForward (Fee (None, None, 768)    4722432     Transformer-12-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-12-FeedForward-Drop (None, None, 768)    0           Transformer-12-FeedForward[0][0]
__________________________________________________________________________________________________
gi (InputLayer)                 (None, 80)           0
__________________________________________________________________________________________________
Transformer-12-FeedForward-Add  (None, None, 768)    0           Transformer-12-MultiHeadSelfAtten
                                                                 Transformer-12-FeedForward-Dropou
__________________________________________________________________________________________________
dense_5 (Dense)                 (None, 80)           6480        gi[0][0]
__________________________________________________________________________________________________
Transformer-12-FeedForward-Norm (None, None, 768)    1536        Transformer-12-FeedForward-Add[0]
__________________________________________________________________________________________________
lambda_2 (Lambda)               (None, 80, 1)        0           dense_5[0][0]
__________________________________________________________________________________________________
agn_1 (AGN)                     [(None, None, 768),  0           Transformer-12-FeedForward-Norm[0
                                                                 lambda_2[0][0]
__________________________________________________________________________________________________
lambda_2 (Lambda)               (None, 80, 1)        0           dense_5[0][0]
__________________________________________________________________________________________________
agn_1 (AGN)                     [(None, None, 768),  0           Transformer-12-FeedForward-Norm[0
                                                                 lambda_2[0][0]
lambda_2 (Lambda)               (None, 80, 1)        0           dense_5[0][0]
__________________________________________________________________________________________________
agn_1 (AGN)                     [(None, None, 768),  0           Transformer-12-FeedForward-Norm[0
                                                                 lambda_2[0][0]
__________________________________________________________________________________________________
agn_1 (AGN)                     [(None, None, 768),  0           Transformer-12-FeedForward-Norm[0
                                                                 lambda_2[0][0]
__________________________________________________________________________________________________
__________________________________________________________________________________________________
lambda_1 (Lambda)               (None, None, 1)      0           Input-Token[0][0]
__________________________________________________________________________________________________
lambda_3 (Lambda)               (None, None, 768)    0           agn_1[0][0]
                                                                 lambda_1[0][0]
__________________________________________________________________________________________________
lambda_1 (Lambda)               (None, None, 1)      0           Input-Token[0][0]
__________________________________________________________________________________________________
lambda_3 (Lambda)               (None, None, 768)    0           agn_1[0][0]
                                                                 lambda_1[0][0]
__________________________________________________________________________________________________
__________________________________________________________________________________________________
lambda_3 (Lambda)               (None, None, 768)    0           agn_1[0][0]
                                                                 lambda_1[0][0]
__________________________________________________________________________________________________
                                                                 lambda_1[0][0]
__________________________________________________________________________________________________
lambda_4 (Lambda)               (None, 768)          0           lambda_3[0][0]
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 768)          0           lambda_4[0][0]
__________________________________________________________________________________________________
dense_8 (Dense)                 (None, 201)          154569      dropout_1[0][0]
==================================================================================================
Total params: 109,052,697
Trainable params: 109,052,697
Non-trainable params: 0
__________________________________________________________________________________________________
apply fgm
start to fitting...
Epoch 1/10
6/6 [==============================] - 208s 35s/step - loss: 5.8743
- val_acc 0.0 - val_f1 0.0
new best model, save model to  save\clf_model.weights...
Epoch 2/10
6/6 [==============================] - 226s 38s/step - loss: 5.8768
- val_acc 0.010416666666666666 - val_f1 0.0027359238699444883
new best model, save model to  save\clf_model.weights...
Epoch 3/10
6/6 [==============================] - 225s 38s/step - loss: 5.7210
- val_acc 0.010416666666666666 - val_f1 0.002333152468800868
Epoch 4/10
6/6 [==============================] - 214s 36s/step - loss: 5.8363
- val_acc 0.010416666666666666 - val_f1 0.0028994845360824743
new best model, save model to  save\clf_model.weights...
Epoch 5/10
6/6 [==============================] - 194s 32s/step - loss: 5.6485
- val_acc 0.010416666666666666 - val_f1 0.00292495403643657
Epoch 6/10
6/6 [==============================] - 186s 31s/step - loss: 5.7115
- val_acc 0.010416666666666666 - val_f1 0.00292495403643657
Epoch 7/10
6/6 [==============================] - 203s 34s/step - loss: 5.7606
- val_acc 0.010416666666666666 - val_f1 0.0029480078613542972
Epoch 8/10
6/6 [==============================] - 212s 35s/step - loss: 5.6432
- val_acc 0.010416666666666666 - val_f1 0.003041225501239018
new best model, save model to  save\clf_model.weights...
Epoch 9/10
6/6 [==============================] - 199s 33s/step - loss: 5.6382
- val_acc 0.010416666666666666 - val_f1 0.0037682524729156855
new best model, save model to  save\clf_model.weights...
Epoch 10/10
6/6 [==============================] - 207s 35s/step - loss: 5.5922
- val_acc 0.010416666666666666 - val_f1 0.0029360967184801382
iteration 1 accuracy: 0.010416666666666666, f1: 0.0037682524729156855

Average accuracy: 0.010416666666666666
Average f1: 0.0037682524729156855